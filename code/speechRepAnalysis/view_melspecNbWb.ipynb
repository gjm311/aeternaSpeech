{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.io.wavfile import read\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import librosa\n",
    "from librosa.feature import melspectrogram\n",
    "import scaleogram as scg \n",
    "from logmmse import logmmse_from_file\n",
    "import pywt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cv2\n",
    "from AEspeech import AEspeech\n",
    "import pdb\n",
    "# from phonetGM2 import Phonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpecDatset import SpecDataset\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.fft\n",
    "import os\n",
    "import sys\n",
    "from CAE import CAEn\n",
    "PATH=os.getcwd()\n",
    "sys.path.append(PATH+\"/toolbox/\")\n",
    "import traintestsplit as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrograms(wav_file,nb,NMELS):\n",
    "        \"\"\"\n",
    "        Compute the tensor of Mel-scale spectrograms to be used as input for the autoencoders from a wav file\n",
    "        :param wav_file: *.wav file with a sampling frequency of 16kHz\n",
    "        :returns: Pytorch tensor (N, C, F, T). N: batch of spectrograms extracted every 500ms, C: number of channels (1),  F: number of Mel frequencies (128), T: time steps (126)\n",
    "        \"\"\"        \n",
    "        FS=16000\n",
    "        FRAME_SIZE=.5\n",
    "        TIME_SHIFT=.25\n",
    "        TIME_STEPS=126\n",
    "            \n",
    "        fs_in, signal=read(wav_file)\n",
    "        sig_len=len(signal)\n",
    "        \n",
    "        #binary narrowband: 1 yes, 0 no (i.e. broadband)\n",
    "        if nb==0:\n",
    "            #broadband: higher time resolution, less frequency resolution\n",
    "            HOP=int(FS*.003)#3ms hop (48 SAMPLES)\n",
    "            NFFT=int(FS*.005)#5ms time window (60 SAMPLES)\n",
    "        elif nb==1:\n",
    "            #narrowband: higher frequency resolution, less time resolution\n",
    "            HOP=int(FS*.01) #10ms hop (160 SAMPLES)\n",
    "            NFFT=int(FS*.03) #30ms time window (480 SAMPLES)\n",
    "        \n",
    "        if fs_in!=FS:\n",
    "            raise ValueError(str(fs)+\" is not a valid sampling frequency\")\n",
    "            \n",
    "        signal=signal-np.mean(signal)\n",
    "        signal=signal/np.max(np.abs(signal))\n",
    "        init=0\n",
    "        endi=int(FRAME_SIZE*FS)\n",
    "        nf=int(len(signal)/(TIME_SHIFT*FS))-1\n",
    "        \n",
    "        if nf>0:\n",
    "            mat=torch.zeros(nf,1,128,TIME_STEPS)\n",
    "            j=0\n",
    "            for k in range(nf):\n",
    "                frame=signal[init:endi]\n",
    "                imag=melspectrogram(frame, sr=16000, n_fft=NFFT, hop_length=HOP, n_mels=NMELS, fmax=FS//2)\n",
    "                imag=imag[np.where(imag[:,0]>0)]\n",
    "                imag=cv2.resize(imag,(126,128),interpolation=cv2.INTER_CUBIC)\n",
    "                imag=np.abs(imag)\n",
    "#                 pdb.set_trace()\n",
    "                #                 imag=(imag-np.min(imag))/(np.max(imag)-np.min(imag))\n",
    "                \n",
    "                init=init+int(TIME_SHIFT*FS)\n",
    "                endi=endi+int(TIME_SHIFT*FS)\n",
    "                if np.min(np.min(imag))<=0:\n",
    "                    warnings.warns(\"There is Inf values in the Mel spectrogram\")\n",
    "                    continue\n",
    "                imag=np.log(imag, dtype=np.float32)\n",
    "                imagt=torch.from_numpy(imag)\n",
    "                mat[j,:,:,:]=imagt\n",
    "                j+=1\n",
    "                \n",
    "#         return torch.reshape(imagt,(1,1,imagt.size()[0],imagt.size()[1])),len(signal)\n",
    "        return mat,len(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrograms(spectrograms1,sig_len,title,nb,spectrograms2=None):\n",
    "    \"\"\"\n",
    "    Visualization of the computed tensor of Mel-scale spectrograms to be used as input for the autoencoders from a wav file\n",
    "    :param spectrograms: tensor of spectrograms obtained from '''compute_spectrograms(wav-file)'''\n",
    "    \"\"\"\n",
    "    if nb==0:\n",
    "        mmax=2595*np.log10(1+8000/700)\n",
    "    else:\n",
    "        mmax=2595*np.log10(1+8000/700)\n",
    "    \n",
    "    m=np.linspace(0,mmax,11)\n",
    "\n",
    "    f=np.round(700*(10**(m/2595)-1))\n",
    "    f=f[::-1]\n",
    "    TIME_STEPS=126\n",
    "    NMELS=128\n",
    "    FS=16000\n",
    "\n",
    "    if torch.is_tensor(spectrograms2):\n",
    "        for k in range(spectrograms1.shape[0]):\n",
    "            fig,(ax1,ax2)=plt.subplots(ncols=2, figsize=(10,10))\n",
    "\n",
    "            mat_curr=spectrograms1.data.numpy()[k,0,:,:]\n",
    "            ax1.imshow(np.flipud(mat_curr), cmap=plt.cm.viridis, vmax=mat_curr.max())\n",
    "            ax1.set_yticks(np.linspace(0,128,11))\n",
    "            ax1.set_yticklabels(map(str, f))\n",
    "            ax1.set_xticks(np.linspace(0,spectrograms.shape[3],6))\n",
    "            ax1.set_xticklabels(map(str, np.linspace(0,sig_len*1000//(FS*TIME_STEPS),6, dtype=np.int)))\n",
    "            ax1.set_ylabel(\"Frequency (Hz)\")\n",
    "            ax1.set_xlabel(\"Time (ms)\")\n",
    "\n",
    "            to_curr=spectrograms2.data.numpy()[k,0,:,:]\n",
    "            ax2.imshow(np.flipud(to_curr), cmap=plt.cm.viridis, vmax=to_curr.max())\n",
    "            ax2.set_yticks(np.linspace(0,NMELS,11))\n",
    "            ax2.set_yticklabels(map(str, f))\n",
    "            ax2.set_xticks(np.linspace(0,spectrograms.shape[3],6))\n",
    "            ax2.set_xticklabels(map(str, np.linspace(0,sig_len*1000//(FS*TIME_STEPS),6, dtype=np.int)))\n",
    "            ax2.set_ylabel(\"Frequency (Hz)\")\n",
    "            ax2.set_xlabel(\"Time (ms)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        spectrograms=spectrograms1\n",
    "\n",
    "        for k in range(spectrograms.shape[0]):\n",
    "            fig,  ax=plt.subplots(1, 1)\n",
    "            fig.set_size_inches(5,3)\n",
    "            mat=spectrograms.data.numpy()[k,0,:,:]\n",
    "            ax.imshow(np.flipud(mat), cmap=plt.cm.viridis, vmax=mat.max())\n",
    "            ax.set_title(title)\n",
    "            ax.set_yticks(np.linspace(0,NMELS,11))\n",
    "            ax.set_yticklabels(map(str, f))\n",
    "            ax.set_xticks(np.linspace(0,spectrograms.shape[3],6))\n",
    "            ax.set_xticklabels(map(str, np.linspace(0,sig_len*1000//(FS*TIME_STEPS),6, dtype=np.int)))\n",
    "            ax.set_ylabel(\"Frequency (Hz)\")\n",
    "            ax.set_xlabel(\"Time (ms)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are wrong or 'pts' directory is incorect...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"C:\\\\Users\\\\Gabriel\\\\Master's Thesis\\\\aeternaSpeech\\\\code\\\\speechRepAnalysis/pts/spec/nb//256_CAE.pt\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-690acc8f638f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0maespeech\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAEspeech\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# mat,sig_len=aespeech.compute_spectrograms(wav_file,nb,nmels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# aespeech.show_spectrograms(mat,sig_len,title,nb)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Master's Thesis\\aeternaSpeech\\code\\speechRepAnalysis\\AEspeech.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, units, rep, nmels, waveletype)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_CAE.pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'wvlt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwvCAEn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"C:\\\\Users\\\\Gabriel\\\\Master's Thesis\\\\aeternaSpeech\\\\code\\\\speechRepAnalysis/pts/spec/nb//256_CAE.pt\""
     ]
    }
   ],
   "source": [
    "rep='spec'\n",
    "mod='CAE'\n",
    "unit=256\n",
    "\n",
    "PATH=os.getcwd()\n",
    "nb=0\n",
    "path_audio=PATH+'/tedx_spanish_corpus/speech/train/'\n",
    "wav_file=path_audio+os.listdir(path_audio)[9]\n",
    "\n",
    "# if nb==1:\n",
    "#     title=\"Narrowband Speech Representation\"\n",
    "#     nmels=128\n",
    "# else:\n",
    "#     title=\"Broadband Speech Representation\"\n",
    "#     nmels=64\n",
    "    \n",
    "    \n",
    "aespeech=AEspeech(model=mod,units=unit,rep=rep)\n",
    "# mat,sig_len=aespeech.compute_spectrograms(wav_file,nb,nmels)\n",
    "# aespeech.show_spectrograms(mat,sig_len,title,nb)\n",
    "mat,sig_len=aespeech.compute_spectrograms(wav_file,nb,nmels)\n",
    "aespeech.show_spectrograms(sig_len,mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH=os.getcwd()\n",
    "# path_audio=PATH+'/tedx_spanish_corpus/speech/train/'\n",
    "# wav_file=path_audio+os.listdir(path_audio)[2]\n",
    "\n",
    "# fs,signal=read(wav_file)\n",
    "# fs_new=16000\n",
    "\n",
    "# if fs!=16000:\n",
    "#     raise ValueError(str(fs)+\" is not a valid sampling frequency\")\n",
    "\n",
    "# NFR=256\n",
    "# FRAME_SIZE=int(signal.shape[0]/NFR)\n",
    "# OVRLP=0.5\n",
    "# TIME_SHIFT=int(FRAME_SIZE*OVRLP)\n",
    "# NBF=128\n",
    "\n",
    "# signal=signal-np.mean(signal)\n",
    "# signal=signal/np.max(np.abs(signal))\n",
    "\n",
    "# init=0\n",
    "# endi=FRAME_SIZE\n",
    "# [freqs,psi,phi]=create_wavelets(FRAME_SIZE,nbf=NBF,dil=1.25)\n",
    "\n",
    "\n",
    "# wv_mat=np.zeros((1,NFR,NBF),dtype=np.float32)\n",
    "# fpsis=np.zeros((NFR,NBF,FRAME_SIZE))\n",
    "# frames=np.zeros((NFR,FRAME_SIZE))\n",
    "\n",
    "# for k in range(NFR):\n",
    "#     frames[k,:]=signal[init:endi]                       \n",
    "#     init=init+int(TIME_SHIFT)\n",
    "#     endi=endi+int(TIME_SHIFT)\n",
    "#     fpsis[k,:,:],fphi=wavelet_transform(frames[k,:],psi,phi)\n",
    "        \n",
    "#     fpw=np.mean(fpsis[k,:,:]**2,axis=1)\n",
    "#     wv_mat[:,k,:]=fpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
